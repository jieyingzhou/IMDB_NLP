{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('../input/movie-data/movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Gensim implementation of Word2Vec. \n",
    "\n",
    "The first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "review_line = []\n",
    "lines = df.review.values.tolist()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # Remove punctuation from each word\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_line.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 50000 review lines in our text corpus. Gensim’s Word2Vec API requires some parameters for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 134156\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "#sentences – List of sentences; here we pass the list of review sentences.\n",
    "\n",
    "#size – The number of dimensions in which we wish to represent our word. This is the size of the word vector.\n",
    "\n",
    "#min_count – Word with frequency greater than min_count only are going to be included into the model. Usually, the bigger and more extensive your text, the higher this number can be.\n",
    "\n",
    "#window – Only terms that occur within a window-neighborhood of a term, in a sentence, are associated with it during training. The usual value is 4 or 5.\n",
    "\n",
    "#workers– Number of threads used in training parallelization, to speed up training\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=review_line, size=embedding_dim, window=5, workers=4, min_count=1)\n",
    "\n",
    "# Vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Word2Vec Model\n",
    "After train the model on IMDB dataset, the vocabulary is 134156.\n",
    "Test the model by using some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excellent', 0.7754549384117126),\n",
       " ('wonderful', 0.7711958885192871),\n",
       " ('fantastic', 0.7553576231002808),\n",
       " ('fine', 0.7369243502616882),\n",
       " ('good', 0.7358146905899048),\n",
       " ('terrific', 0.7053135633468628),\n",
       " ('amazing', 0.6946685910224915),\n",
       " ('brilliant', 0.6699872016906738),\n",
       " ('incredible', 0.6601674556732178),\n",
       " ('awesome', 0.6546913981437683)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romeo', 0.8941930532455444),\n",
       " ('princess', 0.865547239780426),\n",
       " ('samoylova', 0.8610376119613647),\n",
       " ('juliet', 0.8575840592384338),\n",
       " ('prince', 0.8470630049705505),\n",
       " ('ladislaw', 0.8437036871910095),\n",
       " ('bride', 0.8431840538978577),\n",
       " ('angstingwithaninferioritycomplex', 0.8370692133903503),\n",
       " ('tianxia', 0.8325138092041016),\n",
       " ('crimecop', 0.8268693685531616)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# Odd word out\n",
    "print(model.wv.doesnt_match(\"woman king queen movie\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8440559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('boy', 'girl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for the later use\n",
    "The next step is to use the word embeddings directly in the embedding layer in our sentiment classification model. we can save the model to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in ASCII (word2vec) format\n",
    "filename = 'imdb_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pre-trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'imdb_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = X_train + X_test\n",
    "max_length = 260 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next, is to convert the word embedding into tokenized vector. \n",
    "\n",
    "Recall that the review documents are integer encoded prior to passing them to the Embedding layer. The integer maps to the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map to the correct vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134156 unique tokens.\n",
      "Shape of review tensor: (50000, 260)\n",
      "Shape of sentiment tensor: (50000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Vectorize the text \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_line)\n",
    "sequence = tokenizer.texts_to_sequences(review_line)\n",
    "\n",
    "# Pad sequences\n",
    "word_index = tokenizer.word_index\n",
    "review_pad = pad_sequences(sequence, maxlen=max_length)\n",
    "sentiment = df.sentiment.values\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (40000, 260)\n",
      "Shape of y_train tensor: (40000,)\n",
      "Shape of X_test_pad tensor: (10000, 260)\n",
      "Shape of y_test tensor: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "index = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(index)\n",
    "review_pad = review_pad[index]\n",
    "sentiment = sentiment[index]\n",
    "vali_samples = int(0.2 * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-vali_samples]\n",
    "y_train = sentiment[:-vali_samples]\n",
    "X_test_pad = review_pad[-vali_samples:]\n",
    "y_test = sentiment[-vali_samples:]\n",
    "\n",
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now map embeddings from the loaded word2vec model for each word to the tokenizer_obj.word_index vocabulary and create a matrix with of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "We are now ready with the trained embedding vector to be used directly in the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 260, 100)          13415700  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 256, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16385     \n",
      "=================================================================\n",
      "Total params: 13,496,213\n",
      "Trainable params: 80,513\n",
      "Non-trainable params: 13,415,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Load pre-trained word embeddings into an Embedding layer\n",
    "# Note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model uses pre-trained word embedding it has very few trainable params and hence should train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      " - 44s - loss: 0.4578 - acc: 0.7852 - val_loss: 0.3574 - val_acc: 0.8460\n",
      "Epoch 2/25\n",
      " - 44s - loss: 0.3036 - acc: 0.8730 - val_loss: 0.3355 - val_acc: 0.8566\n",
      "Epoch 3/25\n",
      " - 44s - loss: 0.2326 - acc: 0.9056 - val_loss: 0.3540 - val_acc: 0.8470\n",
      "Epoch 4/25\n",
      " - 44s - loss: 0.1727 - acc: 0.9355 - val_loss: 0.3710 - val_acc: 0.8482\n",
      "Epoch 5/25\n",
      " - 43s - loss: 0.1153 - acc: 0.9625 - val_loss: 0.4428 - val_acc: 0.8396\n",
      "Epoch 6/25\n",
      " - 43s - loss: 0.0743 - acc: 0.9787 - val_loss: 0.4675 - val_acc: 0.8428\n",
      "Epoch 7/25\n",
      " - 43s - loss: 0.0467 - acc: 0.9915 - val_loss: 0.5203 - val_acc: 0.8419\n",
      "Epoch 8/25\n",
      " - 43s - loss: 0.0263 - acc: 0.9972 - val_loss: 0.5872 - val_acc: 0.8349\n",
      "Epoch 9/25\n",
      " - 44s - loss: 0.0148 - acc: 0.9994 - val_loss: 0.6147 - val_acc: 0.8371\n",
      "Epoch 10/25\n",
      " - 44s - loss: 0.0089 - acc: 0.9999 - val_loss: 0.6438 - val_acc: 0.8397\n",
      "Epoch 11/25\n",
      " - 44s - loss: 0.0059 - acc: 0.9999 - val_loss: 0.6740 - val_acc: 0.8415\n",
      "Epoch 12/25\n",
      " - 44s - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7053 - val_acc: 0.8403\n",
      "Epoch 13/25\n",
      " - 44s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.7331 - val_acc: 0.8410\n",
      "Epoch 14/25\n",
      " - 44s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7587 - val_acc: 0.8400\n",
      "Epoch 15/25\n",
      " - 44s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.7805 - val_acc: 0.8395\n",
      "Epoch 16/25\n",
      " - 44s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.8062 - val_acc: 0.8391\n",
      "Epoch 17/25\n",
      " - 45s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.8289 - val_acc: 0.8376\n",
      "Epoch 18/25\n",
      " - 46s - loss: 8.6888e-04 - acc: 1.0000 - val_loss: 0.8478 - val_acc: 0.8384\n",
      "Epoch 19/25\n",
      " - 44s - loss: 6.9651e-04 - acc: 1.0000 - val_loss: 0.8700 - val_acc: 0.8380\n",
      "Epoch 20/25\n",
      " - 44s - loss: 5.6798e-04 - acc: 1.0000 - val_loss: 0.8917 - val_acc: 0.8374\n",
      "Epoch 21/25\n",
      " - 44s - loss: 4.6251e-04 - acc: 1.0000 - val_loss: 0.9204 - val_acc: 0.8382\n",
      "Epoch 22/25\n",
      " - 44s - loss: 3.8153e-04 - acc: 1.0000 - val_loss: 0.9361 - val_acc: 0.8381\n",
      "Epoch 23/25\n",
      " - 45s - loss: 3.1305e-04 - acc: 1.0000 - val_loss: 0.9540 - val_acc: 0.8378\n",
      "Epoch 24/25\n",
      " - 45s - loss: 2.5669e-04 - acc: 1.0000 - val_loss: 0.9703 - val_acc: 0.8396\n",
      "Epoch 25/25\n",
      " - 44s - loss: 2.1108e-04 - acc: 1.0000 - val_loss: 0.9945 - val_acc: 0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f033643feb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 381us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "score, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.81%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
